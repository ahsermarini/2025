# Install all packages for the project needed

install.packages("terra")
install.packages("tidyterra")
install.packages("dplyr")
install.packages("geodata")
install.packages("predicts")
install.packages("raster")
install.packages("ggplot2")
install.packages("maps")

# Accessing all required packages 

library(terra)         # Tools for spatial data manipulation (raster/vector)
library(tidyterra)     # Integrates 'terra' with 'tidyverse' for tidy data workflows
library(dplyr)         # Tidyverse package for data manipulation
library(geodata)       # Access and download geospatial/climate data
library(predicts)      # Spatial predictive modeling, e.g., species distribution
library(raster)        # Spatial raster data analysis and modeling
library(ggplot2)       # Data visualization with customizable plots
library(maps)          # World map data for geographic visualization

# Setting up the working directory 
setwd("C:/Users/serma/OneDrive - Alma Mater Studiorum Università di Bologna/Desktop/Invasive_USA")

# loading the dataset from USGS Nonindigenous Aquatic Species
usgs_data <- read.csv("C:/Users/serma/OneDrive - Alma Mater Studiorum Università di Bologna/Desktop/Invasive_USA/BP.csv")

# View the structure of the dataset
str(usgs_data)

# view first few rows of the dataset
head(usgs_data)

# Keeping relevant columns: scientificName, state, longitude, latitude, and year
columns_to_keep <- c("Scientific.Name", "State", "Latitude", "Longitude", "Year")
columns_to_keep <- columns_to_keep[columns_to_keep %in% colnames(usgs_data)] 
filtered_data <- usgs_data[, columns_to_keep]

# Check how many non-numerical or missing values there are for longitude and latitude
non_numeric_long <- sum(is.na(as.numeric(filtered_data$Longitude)))
non_numeric_lat <- sum(is.na(as.numeric(filtered_data$Latitude)))
non_numeric_year <- sum(is.na(as.numeric(filtered_data$Year)))

# Rename name without period
filtered_data <- filtered_data %>% rename("Scientific Name" = Scientific.Name)

# Filter out observations from Ohio (OH)
filtered_data <- filtered_data %>% filter(State != "OH")

# Write cleaned data to CSV
write.csv(filtered_data, file="python_bivittatus_cleaned.csv", row.names = FALSE)

# Download present bioclimatic data
bioclim_present <- worldclim_global(var = "bio", res = 5, path = "data/")

# Download future bioclimatic data
bioclim_future <- cmip6_world(model = "MPI-ESM1-2-HR", ssp = "585", time = "2061-2080", var = "bioc", res = 5, path = "data/")

## Downloading present & future bioclimatic variable data 
bioclim_data <- worldclim_global(var = "bio",
                                 res = 5,
                                 path = "data/")

# Download predicted climate data

forecast_data <- cmip6_world(model = "MPI-ESM1-2-HR",
                             ssp = "585",
                             time = "2061-2080",
                             var = "bioc",
                             res = 5,
                             path = "data")

# Define geographic extent
max_lat <- ceiling(max(filtered_data$Latitude))
min_lat <- floor(min(filtered_data$Latitude))
max_lon <- ceiling(max(filtered_data$Longitude))
min_lon <- floor(min(filtered_data$Longitude))
extent_bounds <- ext(c(min_lon, max_lon, min_lat, max_lat))

# Store boundaries in a single extent object
geographic_extent <- ext(x = c(min_lon, max_lon, min_lat, max_lat))

## Plotting current occurrences on map 

# Downloading data with geodata's world function to use for our base map
world_map <- world(resolution = 3,
                   path = "data/")

# Use the boundaries of the mainland USA for the extent
southeast_extent <- ext(c(-88.0, -80.0, 24.396308, 36.5))

# Cropping the map to the mainland USA extent
my_map <- crop(x = world_map, y = southeast_extent)

# Plotting the base map for BP
plot(my_map,
     axes = TRUE, 
     col = "grey95",
     main = "Occurrence of Python Bivittatus")

# Adding the points for individual observations 
points(x = filtered_data$Longitude, 
       y = filtered_data$Latitude, 
       col = "olivedrab", 
       pch = 20, 
       cex = 0.80)


# Now to prepare the data for climate modeling

# Zoom out by extending the boundaries of the extent
zoomed_out_extent <- ext(c(-95.0, -75.0, 20.0, 40.0))

# Crop both the present and future bioclimatic data to the zoomed-out extent
bioclim_data <- crop(x = bioclim_data, y = zoomed_out_extent)

# Optional: To increase the extent by 25% as per your previous step (if desired)
sample_extent <- southeast_extent * 1.25
bioclim_data <- crop(x = bioclim_data, y = sample_extent)


# Make an extent that is 25% larger
sample_extent <- geographic_extent * 1.25

# Crop bioclim data to desired extent
bioclim_data <- crop(x = bioclim_data, y = sample_extent)

# Setting the seed for the random-number generator to ensure results are similar
set.seed(42)

# Setting the seed for the random-number generator to ensure results are similar
set.seed(42)

# Randomly sample points 
background <- spatSample(x = bioclim_data,
                         size = 10000,
                         values = FALSE, # don't need values
                         na.rm = TRUE,   # don't sample from ocean
                         xy = TRUE)      # just need coordinates

# Pulling out coordinate columns for longitude, then latitude
presence1 <- filtered_data[, c("Longitude", "Latitude")]
# Add column indicating presence
presence1$pa1 <- 1

# Convert background data to a data frame
absence1 <- as.data.frame(background)
# Update column names so they match presence points
colnames(absence1) <- c("Longitude", "Latitude")
# Add column indicating absence
absence1$pa1 <- 0

# Joining data into single data frame
all_points1 <- rbind(presence1, absence1)

# Extracting climate data for all those points
bioclim_extract1 <- extract(x = bioclim_data,
                            y = all_points1[, c("Longitude", "Latitude")],
                            ID = FALSE) 

# Adding the point and climate datasets together
points_climate1 <- cbind(all_points1, bioclim_extract1)

# Identifying columns that are latitude & longitude
drop_cols1 <- which(colnames(points_climate1) %in% c("Longitude", "Latitude"))

# Removing the geographic coordinates from the data frame
points_climate1 <- points_climate1[, -drop_cols1]


# Separating data into training and testing sets
testing1 <- points_climate1[fold1 == 1, ]
training1 <- points_climate1[fold1 != 1, ]

# Model Builiding
# Building a model using training data
glm_model1 <- glm(pa1~., data = training1, family = binomial())

# Getting predicted values from the model
glm_predict1 <- predict(bioclim_data, glm_model1, type = "response")

# Print predicted values 
plot(glm_predict1)

# Use testing data for model evaluation
glm_eval1 <- pa_evaluate(p = testing1[testing1$pa1 == 1, ],
                         a = testing1[testing1$pa1 == 0, ],
                         model = glm_model1,
                         type = "response")

# Determining minimum threshold for "presence"
glm_threshold1 <- glm_eval1@thresholds$max_spec_sens

# Plotting the results
# Plotting base map
plot(my_map, 
     axes = TRUE, 
     col = "grey95",
     main= "")

# Only plotting areas where probability of occurrence is greater than the threshold
plot(glm_predict1 > glm_threshold1, 
     add = TRUE, 
     legend = FALSE, 
     col = c(NA, "olivedrab")) 

# Adding observations
points(x = species_1_data$LONG, 
       y = species_1_data$LAT, 
       col = "black",
       pch = 20, 
       cex = 0.20)

# Redrawing country borders
plot(my_map, add = TRUE, border = "grey5")















# Zoom out by extending the boundaries of the extent
zoomed_out_extent <- ext(c(-95.0, -75.0, 20.0, 40.0))

# Crop both the present and future bioclimatic data to the zoomed-out extent
bioclim_data <- crop(x = bioclim_data, y = zoomed_out_extent)

# Make an extent that is 25% larger
sample_extent <- geographic_extent * 1.25

# Crop bioclim data to desired extent
bioclim_data <- crop(x = bioclim_data, y = sample_extent)

# Psudo-absence point
# Setting the seed for the random-number generator to ensure results are similar
set.seed(42)

# Randomly sample points 
background <- spatSample(x = bioclim_data,
                         size = 10000,
                         values = FALSE, # don't need values
                         na.rm = TRUE,   # don't sample from ocean
                         xy = TRUE)      # just need coordinates

# Pulling out coordinate columns for longitude, then latitude
presence1 <- filtered_data[, c("Longitude", "Latitude")]
# Add column indicating presence
presence1$pa1 <- 1

# Convert background data to a data frame
absence1 <- as.data.frame(background)
# Update column names so they match presence points
colnames(absence1) <- c("Longitude", "Latitude")
# Add column indicating absence
absence1$pa1 <- 0

# Joining data into single data frame
all_points1 <- rbind(presence1, absence1)

# Extracting climate data for all those points
bioclim_extract1 <- extract(x = bioclim_data,
                            y = all_points1[, c("Longitude", "Latitude")],
                            ID = FALSE) 

# Adding the point and climate datasets together
points_climate1 <- cbind(all_points1, bioclim_extract1)

# Identifying columns that are latitude & longitude
drop_cols1 <- which(colnames(points_climate1) %in% c("Longitude", "Latitude"))

# Removing the geographic coordinates from the data frame
points_climate1 <- points_climate1[, -drop_cols1]


# Separating data into training and testing sets
testing1 <- points_climate1[fold1 == 1, ]
training1 <- points_climate1[fold1 != 1, ]

# Model Builiding
# Building a model using training data
glm_model1 <- glm(pa1~., data = training1, family = binomial())

# Getting predicted values from the model
glm_predict1 <- predict(bioclim_data, glm_model1, type = "response")

# Plot predicted values 
plot(glm_predict1)


